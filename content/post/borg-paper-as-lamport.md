+++
date = "2015-05-23T09:59:15+08:00"
draft = true
title = "Borg论文(Larmport版)"

+++

Lamport觉得论文太复杂，说应该用一个故事就讲明白。

## 概要

Borg论文讲了这样一个故事，Google是最牛逼的，Google也在用容器，你们也应该用容器，你们不知道怎么用容器吗？来来来，我给你们讲一下容器调度系统，你们看懂了也不会做？来来来，我给你们做一个。

## 简介

Google一开始是原始社会，有人想做什么事情呢就给他买资源，买了别人可能没用好，从人的本性来讲为了保证自身的利益总是要求比需要更多的资源。然后Google花了大钱买很多服务器，觉得太亏了，不行，我们设计一个叫容器的东西吧，容器有什么好处呢？就是省钱。

Google内部就是个大社会，要求大家都用容器还要有统一的社会管理，然后做了一个名字很吊的系统叫做Borg，它来管理容器。自从用了Borg，人们就不用管服务器硬件了，硬件报废也不用自己处理，资源利用率也上去了。最牛逼的是，我们已经用超过十年了，你们颤抖吧。

## 系统

在新的基于容器的社会里面，程序猿要做的事情就是提交任务也就是Job，一个Job可以包括一个或者多个Task，这些Task运行的代码是相同的，就是为了高可用你懂的。一个Job运行在一系列服务器组成的Cell里。

从古到今计算机有各种各样的程序，可以无脑分为两种，要求马上能响应的在线服务和挂了也无所谓的离线服务。我们就敢于把在线服务和离线服务部署在一起，详情在另外几篇论文可以看到。我们把在线服务成为prod，其他无关紧要的就是no-prod了。这里我们给prod分配70% CPU但只代表了全部CPU的60%？

前面提高一组服务组成的Cell就是一个集群，一个集群当然就只在一个数据中心大楼里面，那多个大类就组成一个Site。为了防止你这个Cell挂了然后我服务都挂了，一般呢我们在一个集群里部一个大的Cell还有一些小的Cell。谦虚一下，中等规模的Cell就一万台服务器，估计很多公司都达不到这个规模就算了。

好了说一个Job有它的名字和一组Task，还有Job可以要求它的运行环境，虽然我不能指定哪一台服务器，不过我可以要求你给我多少个CPU或者我要有外网IP。我们还定义了很多要求，还包括操作系统的版本，软硬件要求都有，非常牛逼吧。然后一个Task就是跑在容器里面的，Task也有它的要求，一般一个Job里面的Task要求是一样的，当然我们是支持某个Task搞特殊的，然后它还有在Job上的编号，表示第几个Task。

用户给我们提交Job，我们得想一种描述Job的方法吧，于是我们拓展GCL做了个BCL。不要问我GCL是什么，它就是生成protobuf文件了，这是个RPC定义文件，有了它我们可以提供命令行工具通过RPC与Borg通信了，更牛逼的是这个GCL支持lambda计算，可以根据环境不同改变配置文件的参数。BCL就是多加了些Borg的东西，如果你还没搞懂，可以看看Aurora的配置文件也差不多。

如果Task是个人，那么他就需要粮食资源，这个就是我们要介绍的资源坑，我们叫做Alloc。一个坑就是一组CPU、内存等资源集合，上面可以跑一个或多个Task，当然这个坑是分配了就不能给别人用了。然后很多台服务器上的坑组成一个Alloc Set，有了这个集合就可以往上面交Task了。

和游戏中的等级一样，每个Task都有一个数字表示他的优先级，这很好理解，等级高的重要可以随便抢等级低的资源。其实这样还不够好，有时两个Task都重要我们不想他们抢占资源，于是我们定义了四个等级分别是Monitoring、Production、Batch和Testing，同一等级的不允许踢掉别人，如果你怕被别人踢掉可以提高些等级。

游戏中等级高的人有特权，例如可以带更多宠物，在Borg里面也是这样的。高优先级的Task有更多更可靠的Quota，他们的Quota分配后马上就可以用了，低优先级的Task也可以申请很多Quota，例如优先级为0的理论上有无限资源，只是经常轮不到分配给它们。我们是用一个向量表示Quota，例如某年某月在某个机房某个优先级有多少个CPU之类的。我们还有管理员账号，后台随便踢人。

其实Borg嘛就是让你们写的代码让我来分配到随便的机器跑，当然跑起来后你得知道你的服务是在哪的，所以我们开发了个BNS也就是Borg Name Service。每个Task都知道它在哪个Cell、是哪个Job的和它的Task编号，这些信息记录在Chubby（就是ZooKeeper或者Etcd）中，当然还有主机名和端口，用户去Chubby就知道服务的地址了。

我们说Task就是一个正在解决问题的人嘛，它就是有没有正常工作还是不小心出Bug没有运行呢，Borg作为管理员必须得知道。于是比较恶心的是几乎每个Task就内嵌个HTTP服务器报告自己是否活着啊、我的性能指标之类的啊。Borg访问这个HTTP服务器就知道Task怎么样了，当然我们还提供一个像Seagull这样的Web管理工具，用户可以看自己Cell的Task和每个Task的资源情况和详细的日志。如无意外这些数据还有整个系统的任务提交记录都会保存到一个地方，Google用的是Dremel，然后运维人员就可以呵呵呵看所有东西了。

## 架构

Borg架构跟黑社会似的，一个老大，多个小弟。老大叫做Borgmaster，每个集群也就是Cell都有一个，一方面是和外面的毒枭打交道，例如现在有批货要到你们那里卖，另一方面管理好自己的小弟做好分销工作（这样写好吗）。事实上老大也是一个组织，由3个人或5个人组成，他们定期选举一个做老大，老大就要处理所有杂七杂八的事情，其他人就闲着，他们闲着就是为了哪一天这个老大挂了可以上位做老大，听起来挺合理的。因为做黑社会老大是随时可能挂的，所以每单生意都必须由这几个人确认过才可以做，当然不是必须全部人都需要确认，他们还用高级的Paxos协议来保持一致呢。

老大接了活后，还得自己来分工，这其实是很有技术含量的，首先是选能干这个活的所有小弟，然后从中选一个最能干的小弟，也就是服务器来运行Task。以前是用E-PVM算法，现在用自己改进的算法来减少分配后不可以用的资源数量，最后的效果是平均的调度时间是25秒，而大部分时间其实是为了培训小弟也就是安装依赖包，如果小弟已经是培训过的也就是安装过的，那就更快了，所以老大会倾向把这些任务发给他们。

在这个黑社会里小弟就叫做Borglet，在每个地区也就是每台服务器上都有一个小弟，他负责执行真正的操作如启停Task等，还要时不时给老大汇报状态。其实更好的做法是老大每几秒就过去问一下，这样老大可以想去就去控制一切，而且为了让黑社会壮大每个老大都负责响应几个小弟然后才内部交流，这样小弟多了加老大来管理也没问题。

这里提一下Google开发了很牛逼的Fauxmaster工具来模拟集群情况，为了调优一个系统开发了另一个系统，你们颤抖吧。

## 隔离

我们知道每个小弟管辖的区业绩是是服务器都有很多生意在做，这些生意之间可能有冲突的，尽量让它们分开。Borg主要就用了chroot和cgroups技术，这里就不详细介绍了。需要注意的是有些生意是外面的人搞的，例如GAE和GCE，这是哦我们是使用KVM虚机来管理了，这样才够安全。

## 其他

论文最后评价了Kubernetes、Mesos和Yarn等框架，还有Kubernetes在Borg基础上的改进，感兴趣的值得看看。



